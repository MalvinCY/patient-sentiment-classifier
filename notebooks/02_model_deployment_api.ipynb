{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27d485fa",
   "metadata": {},
   "source": "# Patient Sentiment Analysis: API Deployment\n\n## Overview\n\nThis notebook deploys the trained LSTM model (73.5% accuracy) as a REST API for real-time sentiment prediction on patient drug reviews.\n\n## Contents\n\n1. Setup and load model\n2. Preprocessing pipeline\n3. Build FastAPI application\n4. Test API locally\n5. Prepare for deployment\n\n## Goals\n\n- Create REST API endpoint for predictions\n- Package preprocessing and model inference together\n- Test locally before cloud deployment\n- Prepare Docker container\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88f5971b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports complete\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Text processing\n",
    "import re\n",
    "import html\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# API framework\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import uvicorn\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbd450f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model architecture defined\n"
     ]
    }
   ],
   "source": [
    "class SentimentLSTM(nn.Module):\n",
    "    \"\"\"LSTM model for 3-class sentiment classification\"\"\"\n",
    "    def __init__(self, embedding_dim=300, hidden_dim=128, num_layers=2, num_classes=3, dropout=0.3):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, (hidden, cell) = self.lstm(x)\n",
    "        final_hidden = hidden[-1]\n",
    "        final_hidden = self.dropout(final_hidden)\n",
    "        output = self.fc(final_hidden)\n",
    "        return output\n",
    "\n",
    "print(\"✓ Model architecture defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a54d458d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "Loading Word2Vec embeddings...\n",
      "Word2Vec loaded\n",
      "\n",
      "Initialising model...\n",
      "Model loaded from ../models/best_lstm_balanced.pth\n",
      "\n",
      "Model has 352,643 parameters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load Word2Vec\n",
    "print(\"\\nLoading Word2Vec embeddings...\")\n",
    "word2vec = api.load(\"word2vec-google-news-300\")\n",
    "print(\"Word2Vec loaded\")\n",
    "\n",
    "# Initialize model\n",
    "print(\"\\nInitialising model...\")\n",
    "model = SentimentLSTM(\n",
    "    embedding_dim=300,\n",
    "    hidden_dim=128,\n",
    "    num_layers=2,\n",
    "    num_classes=3,\n",
    "    dropout=0.3\n",
    ")\n",
    "\n",
    "# Load trained weights\n",
    "model_path = '../models/best_lstm_balanced.pth'\n",
    "if os.path.exists(model_path):\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    print(f\"Model loaded from {model_path}\")\n",
    "else:\n",
    "    print(f\"Model file not found at {model_path}\")\n",
    "    print(\"Please check the path!\")\n",
    "\n",
    "# Test model is loaded\n",
    "print(f\"\\nModel has {sum(p.numel() for p in model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5171dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: This drug worked well but had terrible side effects\n",
      "Cleaned:  drug worked well terrible side effects\n",
      "\n",
      "Embedding shape: (50, 300)\n",
      "\n",
      "✓ Preprocessing pipeline ready\n"
     ]
    }
   ],
   "source": [
    "# Setup NLTK stopwords\n",
    "nltk.download('stopwords', quiet=True)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Preserve negation words (critical for sentiment!)\n",
    "negation_words = {\n",
    "    'no', 'not', 'nor', 'never', 'none', 'nobody', 'nothing', \n",
    "    'neither', 'nowhere', 'hardly', 'scarcely', 'barely',\n",
    "    \"don't\", \"doesn't\", \"didn't\", \"won't\", \"wouldn't\", \"shouldn't\",\n",
    "    \"cannot\", \"can't\", \"couldn't\", \"isn't\", \"aren't\", \"wasn't\", \"weren't\"\n",
    "}\n",
    "stop_words = stop_words - negation_words\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and preprocess review text (same as training)\"\"\"\n",
    "    # Decode HTML entities\n",
    "    text = html.unescape(text)\n",
    "    \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    \n",
    "    # Remove special characters, keep letters/numbers/spaces\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove stop words (keep negations and words > 2 chars)\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens \n",
    "              if (word not in stop_words) and (len(word) > 2 or word in negation_words)]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def review_to_embedding_sequence(review, word2vec_model, max_length=50, embedding_dim=300):\n",
    "    \"\"\"Convert review text to padded embedding sequence\"\"\"\n",
    "    # Preprocess first\n",
    "    cleaned_review = preprocess_text(review)\n",
    "    words = cleaned_review.split()\n",
    "    \n",
    "    # Convert to embeddings\n",
    "    embeddings = []\n",
    "    for word in words[:max_length]:\n",
    "        if word in word2vec_model:\n",
    "            embeddings.append(word2vec_model[word])\n",
    "        else:\n",
    "            embeddings.append(np.zeros(embedding_dim))\n",
    "    \n",
    "    # Pad to max_length\n",
    "    while len(embeddings) < max_length:\n",
    "        embeddings.append(np.zeros(embedding_dim))\n",
    "    \n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Test preprocessing\n",
    "test_review = \"This drug worked well but had terrible side effects\"\n",
    "cleaned = preprocess_text(test_review)\n",
    "print(f\"Original: {test_review}\")\n",
    "print(f\"Cleaned:  {cleaned}\")\n",
    "\n",
    "# Test embedding conversion\n",
    "embedding_seq = review_to_embedding_sequence(test_review, word2vec)\n",
    "print(f\"\\nEmbedding shape: {embedding_seq.shape}\")  # Should be (50, 300)\n",
    "\n",
    "print(\"\\n✓ Preprocessing pipeline ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58ac4273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing predictions:\n",
      "\n",
      "Review: This medication works great with no side effects!\n",
      "Prediction: Positive\n",
      "Probabilities: Neg=0.00, Neu=0.05, Pos=0.95\n",
      "Cleaned: medication works great no side effects\n",
      "--------------------------------------------------------------------------------\n",
      "Review: Terrible drug, caused horrible side effects and didn't help\n",
      "Prediction: Negative\n",
      "Probabilities: Neg=0.95, Neu=0.05, Pos=0.00\n",
      "Cleaned: terrible drug caused horrible side effects help\n",
      "--------------------------------------------------------------------------------\n",
      "Review: The drug worked okay but had some minor side effects\n",
      "Prediction: Positive\n",
      "Probabilities: Neg=0.05, Neu=0.42, Pos=0.53\n",
      "Cleaned: drug worked okay minor side effects\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def predict_sentiment(review_text, model, word2vec_model, device):\n",
    "    \"\"\"\n",
    "    Predict sentiment for a single review\n",
    "    \n",
    "    Args:\n",
    "        review_text: Raw review text (string)\n",
    "        model: Trained LSTM model\n",
    "        word2vec_model: Word2Vec embeddings\n",
    "        device: torch device (cpu/cuda)\n",
    "    \n",
    "    Returns:\n",
    "        dict with prediction, probabilities, and cleaned text\n",
    "    \"\"\"\n",
    "    # Convert to embedding sequence\n",
    "    embedding_seq = review_to_embedding_sequence(review_text, word2vec_model)\n",
    "    \n",
    "    # Convert to tensor and add batch dimension\n",
    "    input_tensor = torch.FloatTensor(embedding_seq).unsqueeze(0)  # (1, 50, 300)\n",
    "    input_tensor = input_tensor.to(device)\n",
    "    \n",
    "    # Predict\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)  # (1, 3)\n",
    "        probabilities = torch.softmax(output, dim=1)  # Convert to probabilities\n",
    "        predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "    \n",
    "    # Map to labels\n",
    "    label_map = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}\n",
    "    \n",
    "    return {\n",
    "        'prediction': label_map[predicted_class],\n",
    "        'prediction_id': predicted_class,\n",
    "        'probabilities': {\n",
    "            'negative': float(probabilities[0][0]),\n",
    "            'neutral': float(probabilities[0][1]),\n",
    "            'positive': float(probabilities[0][2])\n",
    "        },\n",
    "        'cleaned_text': preprocess_text(review_text)\n",
    "    }\n",
    "\n",
    "# Test the prediction function\n",
    "test_reviews = [\n",
    "    \"This medication works great with no side effects!\",\n",
    "    \"Terrible drug, caused horrible side effects and didn't help\",\n",
    "    \"The drug worked okay but had some minor side effects\"\n",
    "]\n",
    "\n",
    "print(\"Testing predictions:\\n\")\n",
    "for review in test_reviews:\n",
    "    result = predict_sentiment(review, model, word2vec, device)\n",
    "    print(f\"Review: {review}\")\n",
    "    print(f\"Prediction: {result['prediction']}\")\n",
    "    print(f\"Probabilities: Neg={result['probabilities']['negative']:.2f}, \"\n",
    "          f\"Neu={result['probabilities']['neutral']:.2f}, \"\n",
    "          f\"Pos={result['probabilities']['positive']:.2f}\")\n",
    "    print(f\"Cleaned: {result['cleaned_text']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e50ad0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ FastAPI application defined\n",
      "\n",
      "API Endpoints:\n",
      "  GET  /          - API info\n",
      "  GET  /health    - Health check\n",
      "  POST /predict   - Sentiment prediction\n",
      "  GET  /docs      - Interactive API documentation\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from typing import Dict\n",
    "\n",
    "# Create FastAPI app\n",
    "app = FastAPI(\n",
    "    title=\"Patient Sentiment Analysis API\",\n",
    "    description=\"Predict sentiment (Negative/Neutral/Positive) for patient drug reviews\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# Define request schema\n",
    "class ReviewRequest(BaseModel):\n",
    "    review: str\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"review\": \"This medication helped with my condition but caused some side effects\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Define response schema\n",
    "class PredictionResponse(BaseModel):\n",
    "    prediction: str\n",
    "    prediction_id: int\n",
    "    probabilities: Dict[str, float]\n",
    "    cleaned_text: str\n",
    "\n",
    "# Health check endpoint\n",
    "@app.get(\"/\")\n",
    "def root():\n",
    "    return {\n",
    "        \"message\": \"Patient Sentiment Analysis API\",\n",
    "        \"status\": \"active\",\n",
    "        \"model\": \"LSTM (73.5% accuracy)\",\n",
    "        \"endpoints\": {\n",
    "            \"health\": \"/health\",\n",
    "            \"predict\": \"/predict (POST)\",\n",
    "            \"docs\": \"/docs\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "@app.get(\"/health\")\n",
    "def health_check():\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"model_loaded\": model is not None,\n",
    "        \"word2vec_loaded\": word2vec is not None\n",
    "    }\n",
    "\n",
    "# Prediction endpoint\n",
    "@app.post(\"/predict\", response_model=PredictionResponse)\n",
    "def predict(request: ReviewRequest):\n",
    "    \"\"\"\n",
    "    Predict sentiment for a patient drug review\n",
    "    \n",
    "    Returns:\n",
    "    - prediction: Sentiment label (Negative/Neutral/Positive)\n",
    "    - prediction_id: Class ID (0/1/2)\n",
    "    - probabilities: Confidence scores for each class\n",
    "    - cleaned_text: Preprocessed review text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = predict_sentiment(\n",
    "            review_text=request.review,\n",
    "            model=model,\n",
    "            word2vec_model=word2vec,\n",
    "            device=device\n",
    "        )\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "print(\"✓ FastAPI application defined\")\n",
    "print(\"\\nAPI Endpoints:\")\n",
    "print(\"  GET  /          - API info\")\n",
    "print(\"  GET  /health    - Health check\")\n",
    "print(\"  POST /predict   - Sentiment prediction\")\n",
    "print(\"  GET  /docs      - Interactive API documentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfc5ed6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing GET /\n",
      "Status: 200\n",
      "Response: {'message': 'Patient Sentiment Analysis API', 'status': 'active', 'model': 'LSTM (73.5% accuracy)', 'endpoints': {'health': '/health', 'predict': '/predict (POST)', 'docs': '/docs'}}\n",
      "\n",
      "Testing GET /health\n",
      "Status: 200\n",
      "Response: {'status': 'healthy', 'model_loaded': True, 'word2vec_loaded': True}\n",
      "\n",
      "Testing POST /predict\n",
      "Status: 200\n",
      "Response: {'prediction': 'Positive', 'prediction_id': 2, 'probabilities': {'negative': 0.002941833809018135, 'neutral': 0.06905363500118256, 'positive': 0.9280045628547668}, 'cleaned_text': 'drug worked amazingly well no side effects'}\n",
      "\n",
      "✓ API tests passed!\n"
     ]
    }
   ],
   "source": [
    "from fastapi.testclient import TestClient\n",
    "\n",
    "# Create test client\n",
    "client = TestClient(app)\n",
    "\n",
    "# Test root endpoint\n",
    "print(\"Testing GET /\")\n",
    "response = client.get(\"/\")\n",
    "print(f\"Status: {response.status_code}\")\n",
    "print(f\"Response: {response.json()}\\n\")\n",
    "\n",
    "# Test health check\n",
    "print(\"Testing GET /health\")\n",
    "response = client.get(\"/health\")\n",
    "print(f\"Status: {response.status_code}\")\n",
    "print(f\"Response: {response.json()}\\n\")\n",
    "\n",
    "# Test prediction endpoint\n",
    "print(\"Testing POST /predict\")\n",
    "test_data = {\n",
    "    \"review\": \"This drug worked amazingly well with no side effects!\"\n",
    "}\n",
    "response = client.post(\"/predict\", json=test_data)\n",
    "print(f\"Status: {response.status_code}\")\n",
    "print(f\"Response: {response.json()}\")\n",
    "\n",
    "print(\"\\n✓ API tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52203ffe",
   "metadata": {},
   "source": "---\n\n## Export API to Standalone File\n\nThe API has been tested and validated in this notebook. Now we export it to a production-ready file."
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8a7b0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ API file created: ../api/app.py\n",
      "  Size: 5,215 bytes\n",
      "  Lines: 163\n",
      "\n",
      "============================================================\n",
      "Next steps:\n",
      "============================================================\n",
      "\n",
      "1. Test the API locally:\n",
      "   Open a terminal and run:\n",
      "   cd patient-sentiment-classifier/api\n",
      "   python app.py\n",
      "\n",
      "2. Visit http://127.0.0.1:8000/docs\n",
      "   (Interactive API documentation)\n",
      "\n",
      "3. Try the /predict endpoint with a review!\n"
     ]
    }
   ],
   "source": [
    "# Test that the API file was created correctly\n",
    "import os\n",
    "\n",
    "api_file = '../api/app.py'\n",
    "if os.path.exists(api_file):\n",
    "    file_size = os.path.getsize(api_file)\n",
    "    print(f\"✓ API file created: {api_file}\")\n",
    "    print(f\"  Size: {file_size:,} bytes\")\n",
    "    print(f\"  Lines: {len(open(api_file).readlines())}\")\n",
    "else:\n",
    "    print(\"❌ API file not found!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Next steps:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n1. Test the API locally:\")\n",
    "print(\"   Open a terminal and run:\")\n",
    "print(\"   cd patient-sentiment-classifier/api\")\n",
    "print(\"   python app.py\")\n",
    "print(\"\\n2. Visit http://127.0.0.1:8000/docs\")\n",
    "print(\"   (Interactive API documentation)\")\n",
    "print(\"\\n3. Try the /predict endpoint with a review!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52b97a8",
   "metadata": {},
   "source": "---\n\n## API Successfully Tested\n\n### What Works\n- Model loads correctly\n- Preprocessing pipeline functional\n- Predictions accurate (87% confidence on test)\n- FastAPI endpoints responsive\n- Interactive docs at /docs\n\n### Files Created\n- `api/app.py` - Production-ready API (163 lines)\n\n### Test Results\n**Input:** \"This medication caused terrible side effects and didn't help at all\"\n**Output:** Negative (87.35% confidence)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}